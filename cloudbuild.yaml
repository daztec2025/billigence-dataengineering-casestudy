steps:
  # Step 1: Build docs Docker image
  - name: "gcr.io/cloud-builders/docker"
    id: "build-docs"
    args:
      - "build"
      - "-t"
      - "gcr.io/$PROJECT_ID/localspark-docs:$COMMIT_SHA"
      - "-t"
      - "gcr.io/$PROJECT_ID/localspark-docs:latest"
      - "-f"
      - "Dockerfile.docs"
      - "."

  # Step 2: Build Spark Docker image
  - name: "gcr.io/cloud-builders/docker"
    id: "build-spark"
    args:
      - "build"
      - "-t"
      - "gcr.io/$PROJECT_ID/localspark-spark:$COMMIT_SHA"
      - "-t"
      - "gcr.io/$PROJECT_ID/localspark-spark:latest"
      - "-f"
      - "Dockerfile.spark"
      - "."

  # Step 3: Push docs image
  - name: "gcr.io/cloud-builders/docker"
    id: "push-docs"
    args: ["push", "--all-tags", "gcr.io/$PROJECT_ID/localspark-docs"]

  # Step 4: Push Spark image
  - name: "gcr.io/cloud-builders/docker"
    id: "push-spark"
    args: ["push", "--all-tags", "gcr.io/$PROJECT_ID/localspark-spark"]

  # Step 5: Deploy docs to Cloud Run Service
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "deploy-docs"
    entrypoint: "gcloud"
    args:
      - "run"
      - "deploy"
      - "localspark-docs"
      - "--image"
      - "gcr.io/$PROJECT_ID/localspark-docs:$COMMIT_SHA"
      - "--region"
      - "us-central1"
      - "--platform"
      - "managed"
      - "--allow-unauthenticated"
      - "--port"
      - "8080"
      - "--memory"
      - "256Mi"
      - "--cpu"
      - "1"
      - "--min-instances"
      - "0"
      - "--max-instances"
      - "2"

  # Step 6: Deploy Spark container as Cloud Run Job
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    id: "deploy-spark-job"
    entrypoint: "bash"
    args:
      - "-c"
      - |
        # Create or update the Cloud Run Job
        gcloud run jobs describe localspark-analysis --region=us-central1 2>/dev/null || \
        gcloud run jobs create localspark-analysis \
          --image=gcr.io/$PROJECT_ID/localspark-spark:$COMMIT_SHA \
          --region=us-central1 \
          --memory=4Gi \
          --cpu=2 \
          --task-timeout=3600s \
          --max-retries=1 \
          --set-env-vars=CASE_STUDY=all

        # Update the job with new image
        gcloud run jobs update localspark-analysis \
          --image=gcr.io/$PROJECT_ID/localspark-spark:$COMMIT_SHA \
          --region=us-central1

options:
  logging: CLOUD_LOGGING_ONLY

images:
  - "gcr.io/$PROJECT_ID/localspark-docs:$COMMIT_SHA"
  - "gcr.io/$PROJECT_ID/localspark-docs:latest"
  - "gcr.io/$PROJECT_ID/localspark-spark:$COMMIT_SHA"
  - "gcr.io/$PROJECT_ID/localspark-spark:latest"
